<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Adaptive Vision Transformer consider the inherent spatial sparsity of event data, using adaptive sampling and adaptive dropout to improve efficiency.">
  <meta name="keywords" content="Event camera, Event-Based Human Pose Estimation, Adaptive Vision Transformer">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Adaptive Vision Transformer for Event-Based Human Pose Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href=" ">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Adaptive Vision Transformer for Event-Based Human Pose Estimatiion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href=" ">Nannan Yu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=" ">Tao Ma</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href=" ">Jiqiing Zhang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href=" ">Yuji Zhang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href=" ">Qirui Bao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href=" ">Xiaopeng Wei</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href=" ">Xin Yang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Dalian University of Technology,</span>
            <span class="author-block"><sup>2</sup>Dalian Marine University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=p4MdxsQVXu"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/doublemanyu/AdaptiveVision-Transformer-for-Event-Based-HPE"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://pan.baidu.com/s/1qLH0kDg7a_K1m2maZFf-iQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Event-based human pose estimation has gained popularity due to the benefits of high temporal resolution and high dynamic range offered by event cameras. 
            The inherent spatial sparsity of event data makes discarding less significant regions a straightforward and effective way to decrease the computation. 
            However, implementing this operation in CNNs poses a challenge, as it disrupts the regularity of dense convolutional workload.
         </p>
          <p>
            In this paper, we propose an adaptive vision transformer, a novel efficient backbone for human pose estimation with event cameras.
            Specifically, we present two adaptive patch and token sampling approaches based on the characteristics of events, thereby reducing the computational load while still achieving comparable performance. 
            Firstly, we design an adaptive patch sampling scheme to eliminate inactivity patches by assessing the entropy of the events before they are inputted into the transformer. 
            Secondly, we further propose an adaptive token reduction strategy to selectively remove less informative tokens in transformer layers through a dynamic token pruning algorithm. 
            To exploit event-based visual cues in human pose estimation tasks, we construct a large-scale frame-event-based dataset, dubbed Event Multi Movement HPE (EventMM HPE). The dataset provides annotation frequencies up to 240 Hz. 
            Extensive experimentsdemonstrate that our proposed approach outperforms existing state-of-the-art methods in estimation accuracy. 
            The source code and dataset are available. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Introduction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
          Event cameras(left) capture more information than traditional cameras(right) under low light scenes
         </p>
        <div class="content has-text-centered">
         <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/lowlight.mp4"
            type="video/mp4">
          </video> 
        </div>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
    

   <section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <div class="column is-centered">
            <img src="./static/images/fig2.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Figure 2: a-c) Prophesee camera, High frame-rate camera and Vicon IR Camera. d) Vicon key positions on the subject and skeleton reputation. e) Schematic of the setup, with Prophesee master camera (position 1), High frame-rate camera (position 2) and Vicon origins
</p>
          </div>
          <p>
            Setup. As shown in Figure 2 (a)-(c), our EventMM HPE dataset is recorded by the Prophesee EVK3 GEN4.1 event camera, with a 1280×720 pixels dynamic vision sensor (DVS). The simultaneous RGB images are recorded by a high-frame-rate camera LUCID ATLAS10 at 120FPS. The Vicon motion capture system with a high sampling rate (up to 330Hz) and sub-millimeter precision, records the 3D coordinates of the subject’s 17 marked joints, which are identified by markers located on the head, neck, spine, left/right shoulders and left/right feet, etc, as shown in Figure 2 (d). Figure 2 (e) illustrates an example of capturing human joint positions using Vicon motion capture system. The synchronization method for all three devices is the same as in DHP19.
         </p>
          <p>
            Collection and Annotation. The dataset collection and annotation are divided into three steps: (i) data collection: markers are placed in the human body, and the three-dimensional spatial positions of joints are captured under the Vicon system, while event data and RGB images are captured using the Prophesee EVK3 GEN4.1 and LUCID ATLAS10, respectively; (ii) event camera intrinsic calibration: the event camera’s intrinsic parameters are first calibrated by capturing images of a calibration board using Zhang’s calibration method; (iii) data annotation: intrinsic parameters will aid in coordinate system transformation between the event camera and Vicon system, facilitating the transformation from 3D points to 2D points.
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->   

       <section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <div class="column is-centered">
            <img src="./static/images/pipeline.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Figure 5: a-c) Framework, Transformer Encode Layer and Adaptive dropout. 
            </p>
          </div>
          <p>
            we introduce our human pose estimation method based on event cameras. 
            To further improve asynchronous and discrete event utilization, we first use Locally-Normalised Event Surfaces (LNES), which retain both events’ spatial information and temporal information. 
            More information was introduced in Section 4.2, where adaptively adjusts the inference cost of vision transformer (ViT) for handling variable event input length. 
            We validate our method with the experimental results on our EventMM HPE dataset in Section 5.
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. -->  

           <section class="section">
  <div class="container is-max-desktop">
    <!-- Dataset. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <div class="column is-centered">
            <img src="./static/images/fig7.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Some Sample Results of our method on EventMM HPE dataset. 
            </p>
          </div>
          <p>
            
          </p>
        </div>
      </div>
    </div>
    <!--/ Dataset. --> 

    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around ours lab.
          </p>
          <p>
            More information please check out the <a href="https://xinyangdut.github.io">homepage</a>.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{yu2024adaptiive,
  author    = {Yu, Nannan and Ma, Tao and Zhang, Jiqing and Zhang, Yuji and Bao, Qirui and Wei, Xiaopeng and Yang, Xin},
  title     = {Adaptive Vision Transformer for Event-Based Human Pose Estimation},
  journal   = {ACM Multiinedia 2024},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://doublemanyu.github.io/Adaptive-Vision-Transformer-for-Event-Based-Human-Pose-Estimatiion.github.io/">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
